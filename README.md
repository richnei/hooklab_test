## Projeto de Web Scraping e API

### üìã Vis√£o Geral

Este projeto implementa um sistema de web scraping para coletar dados de produtos da Amazon e Magazine Luiza, com uma API para consulta dos dados coletados. O sistema utiliza uma arquitetura distribu√≠da com FastAPI, Celery e Redis, tudo containerizado com Docker para garantir portabilidade e facilidade de implanta√ß√£o.

### üèóÔ∏è Arquitetura do Sistema

A arquitetura do sistema √© composta por tr√™s componentes principais:

- API (FastAPI): Interface HTTP para iniciar tarefas de scraping e consultar dados
- Worker (Celery): Processamento ass√≠ncrono das tarefas de scraping
- Broker (Redis): Gerenciamento de filas de tarefas entre a API e os workers

O diagramade arquitetura abaixo ilustra o app:

```mermaid
flowchart TD
    A[Cliente] -->|HTTP Request| B[FastAPI]
    B -->|Enfileira Tarefa| C[Redis]
    C -->|Processa Tarefa| D[Celery Worker]
    D -->|Executa| E[Scrapers]
    E -->|Salva Dados| F[JSON Files]
    F -->|L√™ Dados| B
    B -->|HTTP Response| A
    
    
```

üìÅ Estrutura de Pastas
```

project/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ api.py            # Endpoints da API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Configura√ß√£o principal da aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ tasks.py          # Tarefas ass√≠ncronas do Celery
‚îÇ   ‚îî‚îÄ‚îÄ scrapers/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ base_scraper.py     # Classe base para scrapers
‚îÇ       ‚îú‚îÄ‚îÄ amazon_scraper.py   # Implementa√ß√£o do scraper da Amazon
‚îÇ       ‚îî‚îÄ‚îÄ magalu_scraper.py   # Implementa√ß√£o do scraper da Magazine Luiza
‚îú‚îÄ‚îÄ data/                 # Diret√≥rio onde os dados s√£o salvos
‚îÇ   ‚îú‚îÄ‚îÄ amazon_offers.json
‚îÇ   ‚îî‚îÄ‚îÄ magalu_offers.json
‚îú‚îÄ‚îÄ debug/                # Diret√≥rio para arquivos de debug
‚îú‚îÄ‚îÄ Dockerfile            # Configura√ß√£o do container Docker
‚îú‚îÄ‚îÄ docker-compose.yml    # Configura√ß√£o dos servi√ßos Docker
‚îî‚îÄ‚îÄ requirements.txt      # Depend√™ncias do projeto
```


## üöÄ Como Executar o Projeto
### Pr√©-requisitos
- Docker instalado
- Docker Compose instalado (geralmente inclu√≠do com o Docker Desktop)
  
### Passos para Execu√ß√£o
- Clone o reposit√≥rio
- Inicie os containers com Docker Compose
- docker-compose up --build

## Este comando ir√°:

- Construir as imagens Docker necess√°rias
- Iniciar tr√™s containers:
- web: Servidor FastAPI na porta 8000
- worker: Worker do Celery para processamento ass√≠ncrono
- redis: Broker de mensagens para comunica√ß√£o entre servi√ßos

## Acesse a documenta√ß√£o da API

- Abra seu navegador e acesse:

- http://localhost:8000/docs

- Voc√™ ver√° a interface Swagger UI com todos os endpoints dispon√≠veis.

## Para verificar os logs:

- docker-compose logs -f

## Parando os Containers
- docker-compose down

## üì° Endpoints da API

### Endpoints de Scraping

| M√©todo | Endpoint | Descri√ß√£o |
|--------|----------|-----------|
| POST | `/scrape` | Inicia o scraping de todas as fontes |
| POST | `/scrape/amazon` | Inicia o scraping apenas da Amazon |
| POST | `/scrape/magalu` | Inicia o scraping apenas da Magazine Luiza |

### Endpoints de Consulta

| M√©todo | Endpoint | Descri√ß√£o |
|--------|----------|-----------|
| GET | `/offers` | Retorna todas as ofertas coletadas |
| GET | `/offers/amazon` | Retorna apenas as ofertas da Amazon |
| GET | `/offers/magalu` | Retorna apenas as ofertas da Magazine Luiza |

## üîÑ Fluxo de Execu√ß√£o

```mermaid
sequenceDiagram
    participant Cliente
    participant FastAPI
    participant Redis
    participant Celery
    participant Scrapers
    participant Arquivos

    Cliente->>FastAPI: POST /scrape/amazon
    FastAPI->>Redis: Enfileira tarefa
    Redis->>Celery: Entrega tarefa
    Celery->>Scrapers: Executa scraping
    Scrapers->>Arquivos: Salva dados JSON
    
    Cliente->>FastAPI: GET /offers/amazon
    FastAPI->>Arquivos: L√™ dados JSON
    Arquivos->>FastAPI: Retorna dados
    FastAPI->>Cliente: Resposta JSON
```


## Inicializa√ß√£o:

- Os containers Docker s√£o iniciados
- O servidor FastAPI come√ßa a escutar na porta 8000
- O worker Celery se conecta ao Redis

### Scraping:

- Um cliente faz uma requisi√ß√£o POST para /scrape/amazon
- A API enfileira uma tarefa no Redis
- O worker Celery recebe a tarefa
- O scraper da Amazon √© executado
- Os dados coletados s√£o salvos em data/amazon_offers.json

### Consulta:

- Um cliente faz uma requisi√ß√£o GET para /offers/amazon
- A API l√™ os dados do arquivo data/amazon_offers.json
- A API retorna os dados em formato JSON


## üìä Formato dos Dados

Os dados s√£o salvos em arquivos JSON com a seguinte estrutura:
```
  {
    "nome": "Nome do Produto",
    "preco_vista": "R$ 99,90",
    "preco_prazo": "10x de R$ 9,99",
    "disponivel": true,
    "url": "https://www.exemplo.com/produto",
    "timestamp": "2025-03-30T12:34:56.789"
  }
```

## ‚ö†Ô∏è Considera√ß√µes T√©cnicas
### Desafios de Web Scraping

O projeto implementa t√©cnicas para lidar com os desafios comuns de web scraping:

- Estrutura din√¢mica das p√°ginas: Os scrapers s√£o projetados para serem resilientes a mudan√ßas na estrutura do HTML.
- Prote√ß√µes anti-bot: O scraper da Magazine Luiza pode encontrar CAPTCHAs e outras prote√ß√µes. O sistema salva o HTML para diagn√≥stico.
- Tratamento de erros: Implementa√ß√£o robusta de tratamento de exce√ß√µes para garantir que falhas em um scraper n√£o afetem o sistema como um todo.
  
### Melhorias Potenciais

Em um ambiente de produ√ß√£o, seria recomend√°vel implementar:

- Banco de dados: Substituir o armazenamento em arquivos JSON por um banco de dados como PostgreSQL ou MongoDB.
- T√©cnicas avan√ßadas de scraping: Utilizar Selenium/Playwright para sites com prote√ß√µes anti-bot(ou Agentes de IA).
- Testes automatizados: Implementar testes unit√°rios e de integra√ß√£o para garantir a qualidade do c√≥digo.

## üîç Depura√ß√£o

O sistema inclui recursos para facilitar a depura√ß√£o:

- Logs detalhados: Todos os componentes geram logs informativos.
- Arquivos de debug: O HTML bruto das p√°ginas √© salvo na pasta debug/ para an√°lise.
- Documenta√ß√£o interativa: A interface Swagger permite testar os endpoints diretamente no navegador.

### Para visualizar os logs em tempo real:
```
docker-compose logs -f
```

## üìù Conclus√£o

Este projeto demonstra a implementa√ß√£o de um sistema distribu√≠do para web scraping e disponibiliza√ß√£o de dados via API. A arquitetura escolhida prioriza:

- Escalabilidade: Componentes desacoplados que podem ser escalados independentemente.
- Manutenibilidade: C√≥digo organizado e modular, facilitando extens√µes e modifica√ß√µes.
- Robustez: Tratamento adequado de erros e falhas.
- Facilidade de uso: Documenta√ß√£o clara e interface intuitiva.

A combina√ß√£o de FastAPI, Celery e Docker cria uma solu√ß√£o moderna e eficiente para coleta e disponibiliza√ß√£o de dados de m√∫ltiplas fontes. Uma combina√ß√£o bem robusta de stack.
